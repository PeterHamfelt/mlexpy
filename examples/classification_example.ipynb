{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example `mlxpy` usage on the iris dataset for multi-class classification using a RandomForest and SGDClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load the dataset, models, and mlexpy modules...\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from sklearn.datasets import load_iris\n",
    "from mlexpy import experiment, pipeline_utils, processor\n",
    "\n",
    "from typing import List, Optional, Union, Callable\n",
    "\n",
    "# load a random forest and sgd classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# and numpy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, an example of the general method flow with `mlexpy` (as described in the `README`):\n",
    "1. Load in the dataset\n",
    "2. Create your training and testing set split -- this results in an imutable named tuple structure termed an `ExperimentSetup`, this is made up of 2 `MLSetup` named tuples. Each `MLSetup` named tuple has 2 attributes, a `.obs` attribute,  and a `.labels` attribute. In essence the `.obs` attribute is your feature set (in `mlexpy` this is a pandas DataFrame, and the `.labels` is a pandas Series). An `ExperimentSetup` thus contains an `MLSetup` to use for training, and an `MLSetup` to use _purely_ for testing. This is meant to simply, and in pythonic clear language differentiate the training data (as `ExpiramentSetup.training_data`) and the test data (`ExperimentSetup.test_data`).\n",
    "    - Note: `mlexpy` defers to using a stratified train test split to retain class imbalance / class proporting in training at testing.\n",
    "3. Defing a class to do the data processing / feature engineering that inherits the `mlexpy.processor.ProcessPipelineBase` class; and a class to do the model training that inherits the `mlexpy.expirament.ClassifierExpirament` class. (The explicit notebook cells will better outline this usage.)\n",
    "\n",
    "    - `mlexpy` operates in an object oriented framework. These baseclasses are built to carry a large amount of convieneint, clear, and reproducable behavior.\n",
    "\n",
    "4. Perform your feature engineering, and perform your model training.\n",
    "5. Evaluate your model.\n",
    "6. Store your model (and feature transformation models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm)    0\n",
      "sepal width (cm)     0\n",
      "petal length (cm)    0\n",
      "petal width (cm)     0\n",
      "dtype: int64\n",
      "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
      "count         150.000000        150.000000         150.000000   \n",
      "mean            5.843333          3.057333           3.758000   \n",
      "std             0.828066          0.435866           1.765298   \n",
      "min             4.300000          2.000000           1.000000   \n",
      "25%             5.100000          2.800000           1.600000   \n",
      "50%             5.800000          3.000000           4.350000   \n",
      "75%             6.400000          3.300000           5.100000   \n",
      "max             7.900000          4.400000           6.900000   \n",
      "\n",
      "       petal width (cm)  \n",
      "count        150.000000  \n",
      "mean           1.199333  \n",
      "std            0.762238  \n",
      "min            0.100000  \n",
      "25%            0.300000  \n",
      "50%            1.300000  \n",
      "75%            1.800000  \n",
      "max            2.500000  \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# First, set the random seed(s) for the exprament\n",
    "MODEL_SEED = 10\n",
    "PROCESS_SEED = 100\n",
    "\n",
    "model_rs = np.random.RandomState(MODEL_SEED)\n",
    "process_rs = np.random.RandomState(PROCESS_SEED)\n",
    "\n",
    "# First, read in the dataset as a dataframe. Because mlexpy is meant to be an exploratory/expiramental tool, \n",
    "# dataframes are prefered for their readability.\n",
    "data = load_iris(as_frame=True)\n",
    "features = data[\"data\"]\n",
    "labels = data[\"target\"]\n",
    "\n",
    "# We want to look at the dataset for any faulty records...\n",
    "print(features.isna().sum())\n",
    "print(features.describe())\n",
    "\n",
    "# Spoiler -- there are none in the features. Next look in the labels...\n",
    "print(labels.isna().sum())\n",
    "\n",
    "# Spoiner -- none again, so we use all data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels are 35.33% of the original data (97).\n",
      "Test labels are 64.67% of the original data (53).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, generate the ExpiramentSetup object, that splits the dataset for training and testing.\n",
    "expirament_setup = pipeline_utils.get_stratified_train_test_data(train_data=features, label_data=labels, test_frac=0.35, random_state=process_rs)\n",
    "\n",
    "# This provides us with a named tuple, with attributes of .train_data and .test_data \n",
    "# each one with attributes of .obs and .labels. For example...\n",
    "train_label_count = expirament_setup.train_data.labels.shape[0]\n",
    "test_label_count = expirament_setup.test_data.labels.shape[0]\n",
    "total_data_count = features.shape[0]\n",
    "\n",
    "print(f\"Train labels are {round((total_data_count - train_label_count) / total_data_count * 100, 2)}% of the original data ({train_label_count}).\")\n",
    "print(f\"Test labels are {round((total_data_count - test_label_count) / total_data_count * 100, 2)}% of the original data ({test_label_count}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, define the processing class. This inherits from the `ProcessPipelineBase` class. \n",
    "# For minimal functionality, this class simply needs the `.process_data()` method to be defined. Not adding \n",
    "# code for this class will result in a `NotImplementedError`.\n",
    "\n",
    "# The following shows an example of how to use this class:\n",
    "\n",
    "class IrisPipeline(processor.ProcessPipelineBase):\n",
    "    def __init__(\n",
    "        # All of the Optional arguments are not strictly necessary but shown for brevity.\n",
    "        self, \n",
    "        process_tag: str = \"_development\", \n",
    "        model_dir = None, \n",
    "        model_storage_function = None, \n",
    "        model_loading_function = None\n",
    "        ) -> None:\n",
    "        super().__init__(process_tag, model_dir, model_storage_function, model_loading_function)\n",
    "\n",
    "    # Now -- define the .process_data() method.\n",
    "    def process_data(self, df_i: pd.DataFrame, training: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Now, simply do all feature engineering in this method, and return the final data/feture set to perform\n",
    "        predictions on.\n",
    "\n",
    "        Imagine we have 1 desired feature to engineer, petal/sepal area, and then normalize the features.\n",
    "        We need to pay atenting to the normalizing step, becuase we can ONLY apply the normalize to the test\n",
    "        set, thus we will have a fork in the process when doing the feature normalization. \n",
    "        \n",
    "        In order to easily mainting reproducability in data processing, any model based feature engineering (such\n",
    "        as normalization) is done by creating a specific data structure storing the order from processing each column, \n",
    "        and the model that should be applied. This is somewhat similar to the ColumnProcess in sklearn.\n",
    "\n",
    "        Model based features are handeled in the .fit_model_based_features() method, described below.\n",
    "         \n",
    "        Lets begin:\n",
    "        \"\"\"\n",
    "\n",
    "        # Do a copy of the passed df\n",
    "        df = df_i.copy()\n",
    "\n",
    "        # First, compute the petal / sepal areas (but make the columns simpler)\n",
    "        df.columns = [col.replace(\" \", \"_\").strip(\"_(cm)\") for col in df.columns]\n",
    "\n",
    "        for object in [\"petal\", \"sepal\"]:\n",
    "            df[f\"{object}_area\"] = df[f\"{object}_length\"] * df[f\"{object}_width\"]\n",
    "\n",
    "        # Now perform the training / testing dependent feature processsing. This is why a `training` boolean is passed.\n",
    "\n",
    "        if training:\n",
    "            # Now FIT all of the model based features...\n",
    "            self.fit_model_based_features(df)\n",
    "            # ... and get the results of a transformation of all model based features.\n",
    "            model_features = self.transform_model_based_features(df)\n",
    "        else:\n",
    "            # Here we can ONLY apply the transformation\n",
    "            model_features = self.transform_model_based_features(df)\n",
    "        \n",
    "        # Now, add these 2 dataframes toghert \"horizontaly\"\n",
    "\n",
    "        all_feature_df = pd.concat([df, model_features], axis=1)\n",
    "\n",
    "        # Imagine we only want to use the scaled features for prediction, then we retrieve only the scaled colums.\n",
    "        # (This is easy becuase the columns are renamed with the model name in the column name)\n",
    "\n",
    "        prediction_df = all_feature_df[[col for col in all_feature_df if \"standardscaler\" in col]]\n",
    "\n",
    "        return prediction_df\n",
    "\n",
    "    def fit_model_based_features(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Here we do any processing of columns that will require a model based transformation / engineering.\n",
    "\n",
    "        In this case, simply fit a standard (normalization) scaler to the numerical columns. \n",
    "        This case will result in additional columns on the dataframe named as \n",
    "        \"<original-column-name>_StandardScaler()\".\n",
    "\n",
    "        Note: there are no returned values for this method, the reult is an update in the self.column_transformations dictionary\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype not in (\"float\", \"int\"):\n",
    "                continue\n",
    "            self.fit_scaler(df[column], standard_scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlexpy.processor:No model storage function provided. Using the default class method (joblib, or .store_model native method).\n",
      "INFO:mlexpy.processor:No model loading function provided. Using the default class method (joblib, or .load_model native method).\n",
      "INFO:mlexpy.processor:setting the model path to /Users/NathanSankary/git/mlexpy/examples. (Converting from string to pathlib.Path)\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to sepal_length.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to sepal_width.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to petal_length.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to petal_width.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to petal_area.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to sepal_area.\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to sepal_length\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to sepal_width\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to petal_length\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to petal_width\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to petal_area\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to sepal_area\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length_standardscaler()</th>\n",
       "      <th>sepal_width_standardscaler()</th>\n",
       "      <th>petal_length_standardscaler()</th>\n",
       "      <th>petal_width_standardscaler()</th>\n",
       "      <th>petal_area_standardscaler()</th>\n",
       "      <th>sepal_area_standardscaler()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-0.077948</td>\n",
       "      <td>-0.898694</td>\n",
       "      <td>0.748068</td>\n",
       "      <td>0.898228</td>\n",
       "      <td>0.804410</td>\n",
       "      <td>-0.707603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.809650</td>\n",
       "      <td>-0.898694</td>\n",
       "      <td>0.067794</td>\n",
       "      <td>0.246071</td>\n",
       "      <td>-0.090103</td>\n",
       "      <td>-1.205176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.165953</td>\n",
       "      <td>-0.898694</td>\n",
       "      <td>0.748068</td>\n",
       "      <td>0.506934</td>\n",
       "      <td>0.480863</td>\n",
       "      <td>-0.541745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.541352</td>\n",
       "      <td>0.791648</td>\n",
       "      <td>-1.349444</td>\n",
       "      <td>-1.188672</td>\n",
       "      <td>-1.155906</td>\n",
       "      <td>-0.713746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-0.077948</td>\n",
       "      <td>-1.140172</td>\n",
       "      <td>0.124483</td>\n",
       "      <td>-0.014791</td>\n",
       "      <td>-0.229673</td>\n",
       "      <td>-0.885746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length_standardscaler()  sepal_width_standardscaler()  \\\n",
       "101                      -0.077948                     -0.898694   \n",
       "59                       -0.809650                     -0.898694   \n",
       "83                        0.165953                     -0.898694   \n",
       "6                        -1.541352                      0.791648   \n",
       "92                       -0.077948                     -1.140172   \n",
       "\n",
       "     petal_length_standardscaler()  petal_width_standardscaler()  \\\n",
       "101                       0.748068                      0.898228   \n",
       "59                        0.067794                      0.246071   \n",
       "83                        0.748068                      0.506934   \n",
       "6                        -1.349444                     -1.188672   \n",
       "92                        0.124483                     -0.014791   \n",
       "\n",
       "     petal_area_standardscaler()  sepal_area_standardscaler()  \n",
       "101                     0.804410                    -0.707603  \n",
       "59                     -0.090103                    -1.205176  \n",
       "83                      0.480863                    -0.541745  \n",
       "6                      -1.155906                    -0.713746  \n",
       "92                     -0.229673                    -0.885746  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As an example, lets look at the outputs of the `.process_data()` method.\n",
    "\n",
    "iris_processor = IrisPipeline(model_dir=Path.cwd())  # set the model path to the examples directory\n",
    "\n",
    "# now run the process_data method\n",
    "processed_df = iris_processor.process_data(df_i=expirament_setup.train_data.obs.copy(), training=True)\n",
    "\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, define the IrisExpirament to inherit from the ClassifierExpirament class.\n",
    "#  This functionality works similarily to the `ProcessPipelineBase` class where an expirament is instanciated\n",
    "# as a class, inheriting a variety of expiramental tooling.\n",
    "\n",
    "# We need to define 1 method for our child class, shown below, to handle the training and tes data processing outlined in \n",
    "# our IrisPipeline class\n",
    "\n",
    "class IrisExpirament(experiment.ClassifierExpirament):\n",
    "    def __init__(\n",
    "        self, \n",
    "        train_setup: pipeline_utils.MLSetup, \n",
    "        test_setup: pipeline_utils.MLSetup, \n",
    "        cv_split_count: int, \n",
    "        rnd_int: int = 100, \n",
    "        model_dir: Optional[Union[str, Path]] = None, \n",
    "        model_storage_function: Optional[Callable] = None, \n",
    "        model_loading_function: Optional[Callable] = None, \n",
    "        model_tag: str = \"_development\",\n",
    "        process_tag: str = \"_development\"\n",
    "        ) -> None:\n",
    "        super().__init__(train_setup, test_setup, cv_split_count, rnd_int, model_dir, model_storage_function, model_loading_function, model_tag, process_tag)\n",
    "\n",
    "\n",
    "    def process_data(\n",
    "        self,\n",
    "    ) -> pipeline_utils.ExperimentSetup:\n",
    "\n",
    "        processor = IrisPipeline(process_tag=self.process_tag, model_dir=self.model_dir)\n",
    "\n",
    "        # Here, we do label encoding.\n",
    "        # processor.fit_label_encoder(self.training.labels)\n",
    "        # encoded_training_lables = processor.encode_labels(self.training.labels)\n",
    "        # encoded_testing_lables = processor.encode_labels(self.testing.labels)\n",
    "        encoded_training_lables = self.training.labels\n",
    "        encoded_testing_lables = self.testing.labels\n",
    "\n",
    "        # Now call the .process_data() method we defined above.\n",
    "        train_df = processor.process_data(self.training.obs, training=True)\n",
    "        test_df = processor.process_data(self.testing.obs, training=False)\n",
    "\n",
    "        retained_training_indecies = set(train_df.index).intersection(\n",
    "            encoded_training_lables.index\n",
    "        )\n",
    "        retained_testing_indecies = set(test_df.index).intersection(\n",
    "            encoded_testing_lables.index\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"The train data are of size {train_df.shape}, the test data are {test_df.shape}.\"\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            len(set(train_df.index).intersection(set(test_df.index))) == 0\n",
    "        ), \"There are duplicated indecies in the train and test set.\"\n",
    "\n",
    "        return pipeline_utils.ExperimentSetup(\n",
    "            pipeline_utils.MLSetup(\n",
    "                train_df.loc[list(retained_training_indecies)],\n",
    "                encoded_training_lables.loc[list(retained_training_indecies)],\n",
    "            ),\n",
    "            pipeline_utils.MLSetup(\n",
    "                test_df.loc[list(retained_testing_indecies)],\n",
    "                encoded_testing_lables.loc[list(retained_testing_indecies)],\n",
    "            ),\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "print(len(expirament_setup.train_data.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlexpy.experiment:No model storage function provided. Using the default class method (joblib, or .store_model native method).\n",
      "INFO:mlexpy.experiment:No model loading function provided. Using the default class method (joblib, or .load_model native method).\n",
      "INFO:mlexpy.experiment:setting the model path to /Users/NathanSankary/git/mlexpy/examples. (Converting from string to pathlib.Path)\n",
      "INFO:mlexpy.processor:No model storage function provided. Using the default class method (joblib, or .store_model native method).\n",
      "INFO:mlexpy.processor:No model loading function provided. Using the default class method (joblib, or .load_model native method).\n",
      "INFO:mlexpy.processor:setting the model path to /Users/NathanSankary/git/mlexpy/examples/iris_classification_example_process. (Converting from string to pathlib.Path)\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to sepal_length.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to sepal_width.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to petal_length.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to petal_width.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to petal_area.\n",
      "INFO:mlexpy.processor:Fitting a standard scaler to sepal_area.\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to sepal_length\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to sepal_width\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to petal_length\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to petal_width\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to petal_area\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to sepal_area\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to sepal_length\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to sepal_width\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to petal_length\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to petal_width\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to petal_area\n",
      "INFO:mlexpy.processor:Applying the StandardScaler() to sepal_area\n",
      "INFO:mlexpy.experiment:Performing standard model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data are of size (97, 6), the test data are (53, 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlexpy.experiment:Model trained\n"
     ]
    }
   ],
   "source": [
    "# Now, our \"work\" is done, lets pass our data through this process!\n",
    "\n",
    "# try using a randomforest\n",
    "\n",
    "# Define the expirament\n",
    "expirament = IrisExpirament(\n",
    "    train_setup=expirament_setup.train_data,\n",
    "    test_setup=expirament_setup.test_data,\n",
    "    cv_split_count=20,\n",
    "    model_tag=\"iris_classification_example_model\",\n",
    "    process_tag=\"iris_classification_example_process\",\n",
    "    model_dir=Path.cwd()\n",
    ")\n",
    "\n",
    "# Now begin the expiramentation, start with performing the data processing...\n",
    "processed_datasets = expirament.process_data()\n",
    "\n",
    "# ... then train the model...\n",
    "trained_model = expirament.train_model(\n",
    "    RandomForestClassifier(random_state=model_rs),  # This is why we have 2 different random states...\n",
    "    processed_datasets,\n",
    "    # model_algorithm.hyperparams,  # If this is passed, then cross validation search is performed, but slow.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The f1_macro score is: \n",
      " 0.8848684210526315.\n",
      "\n",
      "The f1_micro score is: \n",
      " 0.8867924528301887.\n",
      "\n",
      "The f1_weighted score is: \n",
      " 0.8856752730883812.\n",
      "\n",
      "The balanced_accruacry score is: \n",
      " 0.8877995642701525.\n",
      "\n",
      "The accuracy score is: \n",
      " 0.8867924528301887.\n",
      "\n",
      "The confusion_matrix score is: \n",
      " [[18  0  0]\n",
      " [ 0 16  1]\n",
      " [ 0  5 13]].\n",
      "\n",
      "The classification_report score is: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.76      0.94      0.84        17\n",
      "           2       0.93      0.72      0.81        18\n",
      "\n",
      "    accuracy                           0.89        53\n",
      "   macro avg       0.90      0.89      0.88        53\n",
      "weighted avg       0.90      0.89      0.89        53\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Now, evalute the predictions, ClassificationExpiramentBase provides some standard classification metrics\n",
    "# and evaluations.\n",
    "\n",
    "# Get the predictions and evaluate the performance.\n",
    "predictions = expirament.predict(processed_datasets, trained_model)\n",
    "results =expirament.evaluate_predictions(processed_datasets, predictions=predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('mlexpy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a3bf03e9bc503568ad4247b4207894faa64f010818fdf63117cea0fa36d0f36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
